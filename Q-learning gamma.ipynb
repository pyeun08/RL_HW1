{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150881b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_arrows(q_table, grid_size=6):\n",
    "    action_to_arrow = ['turn left  ', 'turn right ', 'go straight']\n",
    "    policy = [[\"\" for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "\n",
    "    for state, q_vals in q_table.items():\n",
    "        state //= 3\n",
    "        x = state % grid_size\n",
    "        y = state // grid_size\n",
    "\n",
    "        best_action = np.argmax(q_vals)\n",
    "        policy[y][x] = action_to_arrow[best_action]\n",
    "    \n",
    "    policy[grid_size - 1][grid_size - 1] = 'Terminal'\n",
    "\n",
    "    for row in policy:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def plot_rewards_for_all_envs(rewards_g01, rewards_g05, rewards_g09, rewards_g099, window=100):\n",
    "    avg_g01 = np.convolve(rewards_g01, np.ones(window)/window, mode='valid')\n",
    "    avg_g05 = np.convolve(rewards_g05, np.ones(window)/window, mode='valid')\n",
    "    avg_g09 = np.convolve(rewards_g09, np.ones(window)/window, mode='valid')\n",
    "    avg_g099 = np.convolve(rewards_g099, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    # Plotting for all environments\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # env_g01\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(avg_g01, label=\"Q-learning\", color='blue')\n",
    "    plt.title(\"env_g01\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Moving Avg Reward ({window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # env_g05\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(avg_g05, label=\"Q-learning\", color='blue')\n",
    "    plt.title(\"env_g05\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Moving Avg Reward ({window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # env_g09\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(avg_g09, label=\"Q-learning\", color='blue')\n",
    "    plt.title(\"env_g09\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Moving Avg Reward ({window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # env_g099\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(avg_g099, label=\"Q-learning\", color='blue')\n",
    "    plt.title(\"env_g099\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(f\"Moving Avg Reward ({window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def Summary_of_experimental_settings(num_iter, alpha):\n",
    "    params = {\n",
    "        'Agent': ['Q-learning', 'SARSA'],\n",
    "        'Alpha (Learning rate)': [alpha, alpha],\n",
    "        'Gamma (Discount factor)': [0.9, 0.9],\n",
    "        'Epsilon (Exploration)': [0.2, 0.2],\n",
    "        'Environment': ['MiniGrid-Empty-6x6-v0', 'MiniGrid-Empty-6x6-v0'],\n",
    "        'Episodes': [num_iter, num_iter]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(params)\n",
    "    print(\"Experimental Settings\")\n",
    "    print(df.to_markdown(index=False))  # 깔끔한 콘솔 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3475c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, actions, gamma, agent_indicator=10):\n",
    "        self.actions = actions\n",
    "        self.agent_indicator = agent_indicator\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 0.2\n",
    "        self.q_values = defaultdict(lambda: [0.0] * actions)\n",
    "        \n",
    "    def _convert_state(self, s):\n",
    "        return np.where(s == self.agent_indicator)[0][0]\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = self._convert_state(state)\n",
    "        next_state = self._convert_state(next_state)\n",
    "        next_action = np.argmax(self.q_values[next_state])\n",
    "        \n",
    "        q_value = self.q_values[state][action]\n",
    "        \n",
    "        next_q_value = self.q_values[next_state][next_action]\n",
    "        \n",
    "        td_error = reward + self.gamma * next_q_value - q_value\n",
    "        self.q_values[state][action] = q_value + self.alpha * td_error\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = random.randint(0, self.actions - 1)\n",
    "        else:\n",
    "            state = self._convert_state(state)\n",
    "            q_values = self.q_values[state]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90706d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"D:/대학/자료/4-1/강화학습/과제/과제 1/Sarsa&Q-learning\")\n",
    "\n",
    "from my_env_utils import gen_wrapped_env, show_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a62600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # 현재 디렉토리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048dbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_g01 = gen_wrapped_env('MiniGrid-Empty-6x6-v0')\n",
    "env_g05 = gen_wrapped_env('MiniGrid-Empty-6x6-v0')\n",
    "env_g09 = gen_wrapped_env('MiniGrid-Empty-6x6-v0')\n",
    "env_g099 = gen_wrapped_env('MiniGrid-Empty-6x6-v0')\n",
    "\n",
    "obs_g01 = env_g01.reset()\n",
    "obs_g05 = env_g05.reset()\n",
    "obs_g09 = env_g09.reset()\n",
    "obs_g099 = env_g099.reset()\n",
    "\n",
    "agent_g01_position = obs_g01[0]\n",
    "agent_g05_position = obs_g05[0]\n",
    "agent_g09_position = obs_g09[0]\n",
    "agent_g099_position = obs_g099[0]\n",
    "\n",
    "agent_g01 = QLearning(3, gamma=0.1, agent_indicator=agent_g01_position)\n",
    "agent_g05 = QLearning(3, gamma=0.5, agent_indicator=agent_g05_position)\n",
    "agent_g09 = QLearning(3, gamma=0.9, agent_indicator=agent_g09_position)\n",
    "agent_g099 = QLearning(3, gamma=0.99, agent_indicator=agent_g099_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8c55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rewards_g01 = []\n",
    "rewards_g05 = []\n",
    "rewards_g09 = []\n",
    "rewards_g099 = []\n",
    "\n",
    "num_iter = 3000\n",
    "\n",
    "for ep in range(num_iter):\n",
    "    done_g01 = done_g05 = done_g09 = done_g099 = False\n",
    "    obs_g01 = env_g01.reset()\n",
    "    obs_g05 = env_g05.reset()\n",
    "    obs_g09 = env_g09.reset()\n",
    "    obs_g099 = env_g099.reset()\n",
    "    \n",
    "    action_g01 = agent_g01.act(obs_g01)\n",
    "    action_g05 = agent_g05.act(obs_g05)\n",
    "    action_g09 = agent_g09.act(obs_g09)\n",
    "    action_g099 = agent_g099.act(obs_g099)\n",
    "    \n",
    "    ep_rewards_g01 = ep_rewards_g05 = ep_rewards_g09 = ep_rewards_g099 = 0\n",
    "\n",
    "    while not (done_g01 and done_g05 and done_g09 and done_g099):\n",
    "        # Update each environment separately\n",
    "        if not done_g01:\n",
    "            next_obs_g01, reward_g01, done_g01, _ = env_g01.step(action_g01)\n",
    "            agent_g01.update(obs_g01, action_g01, reward_g01, next_obs_g01)\n",
    "            action_g01 = agent_g01.act(next_obs_g01)\n",
    "            ep_rewards_g01 += reward_g01\n",
    "            obs_g01 = next_obs_g01\n",
    "\n",
    "        if not done_g05:\n",
    "            next_obs_g05, reward_g05, done_g05, _ = env_g05.step(action_g05)\n",
    "            agent_g05.update(obs_g05, action_g05, reward_g05, next_obs_g05)\n",
    "            action_g05 = agent_g05.act(next_obs_g05)\n",
    "            ep_rewards_g05 += reward_g05\n",
    "            obs_g05 = next_obs_g05\n",
    "\n",
    "        if not done_g09:\n",
    "            next_obs_g09, reward_g09, done_g09, _ = env_g09.step(action_g09)\n",
    "            agent_g09.update(obs_g09, action_g09, reward_g09, next_obs_g09)\n",
    "            action_g09 = agent_g09.act(next_obs_g09)\n",
    "            ep_rewards_g09 += reward_g09\n",
    "            obs_g09 = next_obs_g09\n",
    "\n",
    "        if not done_g099:\n",
    "            next_obs_g099, reward_g099, done_g099, _ = env_g099.step(action_g099)\n",
    "            agent_g099.update(obs_g099, action_g099, reward_g099, next_obs_g099)\n",
    "            action_g099 = agent_g099.act(next_obs_g099)\n",
    "            ep_rewards_g099 += reward_g099\n",
    "            obs_g099 = next_obs_g099\n",
    "\n",
    "    rewards_g01.append(ep_rewards_g01)\n",
    "    rewards_g05.append(ep_rewards_g05)\n",
    "    rewards_g09.append(ep_rewards_g09)\n",
    "    rewards_g099.append(ep_rewards_g099)\n",
    "    \n",
    "    if (ep+1) % 100 == 0:\n",
    "        avg_g01 = np.mean(rewards_g01[-100:])\n",
    "        avg_g05 = np.mean(rewards_g05[-100:])\n",
    "        avg_g09 = np.mean(rewards_g09[-100:])\n",
    "        avg_g099 = np.mean(rewards_g099[-100:])\n",
    "        print(f\"[Episode {ep+1}] Q avg (env_g01): {avg_g01:.2f}, (env_g05): {avg_g05:.2f}, (env_g09): {avg_g09:.2f}, (env_g099): {avg_g099:.2f}\")\n",
    "\n",
    "env_g01.close()\n",
    "env_g05.close()\n",
    "env_g09.close()\n",
    "env_g099.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42787cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 환경에 대한 보상을 저장\n",
    "pd.Series(rewards_g01).to_csv('./logs/rewards_qlearning_env_g01.csv', index=False)\n",
    "pd.Series(rewards_g05).to_csv('./logs/rewards_qlearning_env_g05.csv', index=False)\n",
    "pd.Series(rewards_g09).to_csv('./logs/rewards_qlearning_env_g09.csv', index=False)\n",
    "pd.Series(rewards_g099).to_csv('./logs/rewards_qlearning_env_g099.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da141489",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_logs01 = pd.read_csv('./logs/rewards_qlearning_env_g01.csv', index_col=False).iloc[:, 1]\n",
    "g_logs05 = pd.read_csv('./logs/rewards_qlearning_env_g05.csv', index_col=False).iloc[:, 1]\n",
    "g_logs09 = pd.read_csv('./logs/rewards_qlearning_env_g09.csv', index_col=False).iloc[:, 1]\n",
    "g_logs099 = pd.read_csv('./logs/rewards_qlearning_env_g099.csv', index_col=False).iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# 각 환경의 누적 보상 계산 후 그래프에 추가\n",
    "plt.plot(g_logs01.cumsum() / (pd.Series(np.arange(g_logs01.shape[0]))+1), label=\"env_g01 (gamma = 0.1)\")\n",
    "plt.plot(g_logs05.cumsum() / (pd.Series(np.arange(g_logs05.shape[0]))+1), label=\"env_g05 (gamma = 0.1)\")\n",
    "plt.plot(g_logs09.cumsum() / (pd.Series(np.arange(g_logs09.shape[0]))+1), label=\"env_g09 (gamma = 0.1)\")\n",
    "plt.plot(g_logs099.cumsum() / (pd.Series(np.arange(g_logs099.shape[0]))+1), label=\"env_g099 (gamma = 0.1)\")\n",
    "\n",
    "# 레이블 및 범례 추가\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Cumulative Reward')\n",
    "plt.title('Cumulative Reward Over Episodes (Different Environments)')\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning 결과 출력 및 정책 화살표 그리기\n",
    "print('Q-learning for env_g01')\n",
    "plot_policy_arrows(agent_g01.q_values, grid_size=4)\n",
    "\n",
    "print('Q-learning for env_g05')\n",
    "plot_policy_arrows(agent_g05.q_values, grid_size=4)\n",
    "\n",
    "print('Q-learning for env_g09')\n",
    "plot_policy_arrows(agent_g09.q_values, grid_size=4)\n",
    "\n",
    "print('Q-learning for env_g099')\n",
    "plot_policy_arrows(agent_g099.q_values, grid_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보상 그래프 그리기\n",
    "plot_rewards_for_all_envs(rewards_g01, rewards_g05, rewards_g09, rewards_g099, window=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_env)",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
